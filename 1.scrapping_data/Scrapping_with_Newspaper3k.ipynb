{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnbqKvkHbQuH"
   },
   "source": [
    "# *Installation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFyNjl56bP-A",
    "outputId": "c6360190-ccea-4b29-ee93-e612ec33f1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3K in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (0.35.1)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (4.4.1)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (3.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (2.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (2.8.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (4.8.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (5.1.2)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (3.4.5)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (0.0.4)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (0.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (6.2.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (6.0.2)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3K) (3.0.12)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3K) (2.8)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3K) (1.5.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3K) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3K) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3K) (1.25.11)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.3->newspaper3K) (1.12.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3K) (1.9.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3K) (1.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3K \n",
    "!pip install nltk        \n",
    "!pip install textblob    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xYDx2yU_GEZ"
   },
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gq4-ctkX_JC2"
   },
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import Source\n",
    "from newspaper import news_pool\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ShAV9nlR_NAn"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRiV1d8FEJDo"
   },
   "source": [
    "# Scrapping  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B83CfL4ebl83"
   },
   "source": [
    "# 1. Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uEUgLsq-bk6o"
   },
   "outputs": [],
   "source": [
    "# The various News Sources we will like to web scrape from\n",
    "who = newspaper.build(\"https://www.who.int/home\", memoize_articles=False)\n",
    "sc1 = newspaper.build(\"https://www.cdc.gov/coronavirus/2019-ncov/index.html\", memoize_articles=False)\n",
    "sc2 = newspaper.build(\"https://www.defense.gov/explore/spotlight/coronavirus/rumor-control/\", memoize_articles=False)\n",
    "sc3 = newspaper.build(\"https://www.cdc.gov/coronavirus/2019-ncov/if-you-are-sick/quarantine.html\", memoize_articles=False)\n",
    "sc4 = newspaper.build(\"https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/index.html\", memoize_articles=False)\n",
    "sc5 = newspaper.build(\"https://www.bbc.com/news/coronavirus\", memoize_articles=False)\n",
    "sc6 = newspaper.build(\"https://www.bbc.com/news/have_your_say\", memoize_articles=False)\n",
    "# Place the sources in a list\n",
    "papers = [who,sc1,sc2,sc3,sc4,sc5,sc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVjcvbRlbBb1",
    "outputId": "bcad8443-06e1-471e-a20b-a59f1e81a057"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\jieba\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\Lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.365577220916748 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Essentially you will be downloading 4 articles parallely per source.\n",
    "# Since we have two sources, that means 8 articles are downloaded at any one time. \n",
    "# Greatly speeding up the processes.\n",
    "# Once downloaded it will be stored in memory to be used in the for loop below \n",
    "# to extract the bits of data we want.\n",
    "news_pool.set(papers, threads_per_source=4)\n",
    "\n",
    "news_pool.join()\n",
    "\n",
    "# Create our final dataframe\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Create a download limit per sources\n",
    "# NOTE: You may not want to use a limit\n",
    "# limit = 100\n",
    "\n",
    "for source in papers:\n",
    "    # temporary lists to store each element we want to extract\n",
    "    list_title = []\n",
    "    list_text = []\n",
    "    list_source =[]\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for article_extract in source.articles:\n",
    "        article_extract.parse()\n",
    "\n",
    "        # if count > limit: # Lets have a limit, so it doesnt take too long when you're\n",
    "        #     break         # running the code. NOTE: You may not want to use a limit\n",
    "\n",
    "        # Appending the elements we want to extract\n",
    "        #list_title.append(article_extract.title)\n",
    "        list_text.append(article_extract.text)\n",
    "        list_source.append(article_extract.source_url)\n",
    "\n",
    "        # Update count\n",
    "        count +=1\n",
    "\n",
    "\n",
    "    temp_df = pd.DataFrame({'Text': list_text, 'Source': list_source})\n",
    "    # Append to the final DataFrame\n",
    "    final_df = final_df.append(temp_df, ignore_index = True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_ow6_ZkgYIZ",
    "outputId": "8946a673-a1e1-43a5-bce2-d3937ecd8702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "GSW5JQg07pTz",
    "outputId": "0eac3a30-5399-4a7f-9b5d-5c7e83970319"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>When autocomplete results are available use up...</td>\n",
       "      <td>https://www.who.int/countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020 2019 2018 2017 2016 2015 2014 2013 2012 2...</td>\n",
       "      <td>https://www.who.int/countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Statement to the resumed 73rd World Health Ass...</td>\n",
       "      <td>https://www.who.int/countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Global public health days offer great potentia...</td>\n",
       "      <td>https://www.who.int/countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>When autocomplete results are available use up...</td>\n",
       "      <td>https://www.who.int/countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1196</td>\n",
       "      <td>People across the world are celebrating one of...</td>\n",
       "      <td>https://www.bbc.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1197</td>\n",
       "      <td>Hawaiian shirts and gravy: Christmas tributes ...</td>\n",
       "      <td>https://www.bbc.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1198</td>\n",
       "      <td>Limited by how far she could travel for a pict...</td>\n",
       "      <td>https://www.bbc.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>M8 Glasgow City eastbound severe disruption, f...</td>\n",
       "      <td>https://www.bbc.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>It might be Christmas, but for keen stargazers...</td>\n",
       "      <td>https://www.bbc.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0     When autocomplete results are available use up...   \n",
       "1     2020 2019 2018 2017 2016 2015 2014 2013 2012 2...   \n",
       "2     Statement to the resumed 73rd World Health Ass...   \n",
       "3     Global public health days offer great potentia...   \n",
       "4     When autocomplete results are available use up...   \n",
       "...                                                 ...   \n",
       "1196  People across the world are celebrating one of...   \n",
       "1197  Hawaiian shirts and gravy: Christmas tributes ...   \n",
       "1198  Limited by how far she could travel for a pict...   \n",
       "1199  M8 Glasgow City eastbound severe disruption, f...   \n",
       "1200  It might be Christmas, but for keen stargazers...   \n",
       "\n",
       "                             Source  \n",
       "0     https://www.who.int/countries  \n",
       "1     https://www.who.int/countries  \n",
       "2     https://www.who.int/countries  \n",
       "3     https://www.who.int/countries  \n",
       "4     https://www.who.int/countries  \n",
       "...                             ...  \n",
       "1196            https://www.bbc.com  \n",
       "1197            https://www.bbc.com  \n",
       "1198            https://www.bbc.com  \n",
       "1199            https://www.bbc.com  \n",
       "1200            https://www.bbc.com  \n",
       "\n",
       "[1201 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-ZeEp-AXgUyh"
   },
   "outputs": [],
   "source": [
    "# From here you can export this to csv file\n",
    "final_df.to_csv(r'../dataset/1.newspaper_scraper_english.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rwbnDC7EET0g",
    "vCt51hcfd9hO",
    "Vkmk_4m-ucnA"
   ],
   "name": "Scrapping_with_Newspaper3k.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
