{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnbqKvkHbQuH"
   },
   "source": [
    "# *Installation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFyNjl56bP-A",
    "outputId": "c6360190-ccea-4b29-ee93-e612ec33f1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3K in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (0.35.1)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (4.4.1)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (3.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (2.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (2.8.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (4.8.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (5.1.2)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (3.4.5)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (0.0.4)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (0.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (6.2.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from newspaper3K) (6.0.2)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3K) (3.0.12)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3K) (2.8)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3K) (1.5.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3K) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3K) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3K) (1.25.11)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.3->newspaper3K) (1.12.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3K) (1.9.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3K) (1.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3K \n",
    "!pip install nltk        \n",
    "!pip install textblob    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xYDx2yU_GEZ"
   },
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gq4-ctkX_JC2"
   },
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import Source\n",
    "from newspaper import news_pool\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ShAV9nlR_NAn"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRiV1d8FEJDo"
   },
   "source": [
    "# Scrapping  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B83CfL4ebl83"
   },
   "source": [
    "# 1. Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uEUgLsq-bk6o"
   },
   "outputs": [],
   "source": [
    "# The various News Sources we will like to web scrape from\n",
    "who = newspaper.build(\"https://www.who.int/home\", memoize_articles=False)\n",
    "sc1 = newspaper.build(\"https://www.cdc.gov/coronavirus/2019-ncov/index.html\", memoize_articles=False)\n",
    "sc2 = newspaper.build(\"https://www.defense.gov/explore/spotlight/coronavirus/rumor-control/\", memoize_articles=False)\n",
    "sc3 = newspaper.build(\"https://www.cdc.gov/coronavirus/2019-ncov/if-you-are-sick/quarantine.html\", memoize_articles=False)\n",
    "sc4 = newspaper.build(\"https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/index.html\", memoize_articles=False)\n",
    "sc5 = newspaper.build(\"https://www.bbc.com/news/coronavirus\", memoize_articles=False)\n",
    "sc6 = newspaper.build(\"https://www.bbc.com/news/have_your_say\", memoize_articles=False)\n",
    "# Place the sources in a list\n",
    "papers = [who,sc1,sc2,sc3,sc4,sc5,sc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVjcvbRlbBb1",
    "outputId": "bcad8443-06e1-471e-a20b-a59f1e81a057"
   },
   "outputs": [],
   "source": [
    "# Essentially you will be downloading 4 articles parallely per source.\n",
    "# Since we have two sources, that means 8 articles are downloaded at any one time. \n",
    "# Greatly speeding up the processes.\n",
    "# Once downloaded it will be stored in memory to be used in the for loop below \n",
    "# to extract the bits of data we want.\n",
    "news_pool.set(papers, threads_per_source=4)\n",
    "\n",
    "news_pool.join()\n",
    "\n",
    "# Create our final dataframe\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Create a download limit per sources\n",
    "# NOTE: You may not want to use a limit\n",
    "# limit = 100\n",
    "\n",
    "for source in papers:\n",
    "    # temporary lists to store each element we want to extract\n",
    "    list_title = []\n",
    "    list_text = []\n",
    "    list_source =[]\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for article_extract in source.articles:\n",
    "        article_extract.parse()\n",
    "\n",
    "        # if count > limit: # Lets have a limit, so it doesnt take too long when you're\n",
    "        #     break         # running the code. NOTE: You may not want to use a limit\n",
    "\n",
    "        # Appending the elements we want to extract\n",
    "        #list_title.append(article_extract.title)\n",
    "        list_text.append(article_extract.text)\n",
    "        list_source.append(article_extract.source_url)\n",
    "\n",
    "        # Update count\n",
    "        count +=1\n",
    "\n",
    "\n",
    "    temp_df = pd.DataFrame({'Text': list_text, 'Source': list_source})\n",
    "    # Append to the final DataFrame\n",
    "    final_df = final_df.append(temp_df, ignore_index = True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_ow6_ZkgYIZ",
    "outputId": "8946a673-a1e1-43a5-bce2-d3937ecd8702"
   },
   "outputs": [],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "GSW5JQg07pTz",
    "outputId": "0eac3a30-5399-4a7f-9b5d-5c7e83970319"
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZeEp-AXgUyh"
   },
   "outputs": [],
   "source": [
    "# From here you can export this to csv file\n",
    "final_df.to_csv('1.newspaper_scraper_english.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rwbnDC7EET0g",
    "vCt51hcfd9hO",
    "Vkmk_4m-ucnA"
   ],
   "name": "Scrapping_with_Newspaper3k.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
